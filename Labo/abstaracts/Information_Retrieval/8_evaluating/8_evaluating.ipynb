{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6e6bb8e",
   "metadata": {},
   "source": [
    "元url(https://nlp.stanford.edu/IR-book/pdf/08eval.pdf)  \n",
    "情報の需要と検索エンジンについての話。  \n",
    "crawling: 様々なサイトを巡回して情報の保存を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a6eb7b",
   "metadata": {},
   "source": [
    "## 8.1 情報検索システムの評価（Information Retrieval System Evaluation）\n",
    "\n",
    "---\n",
    "\n",
    "### 🔸 評価の目的\n",
    "\n",
    "- 情報検索（IR）システムには多くの設計選択肢がある（例：ステミング、ストップワード、IDFの使用など）。\n",
    "- どの設計が有効かを比較・検証するには、**定量的な評価**が必要。\n",
    "- 評価はシステムの**有効性（effectiveness）**を測定する手段。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔸 評価に必要な3要素（テストコレクション）\n",
    "\n",
    "1. **文書コレクション（Document Collection）**\n",
    "2. **情報ニーズ（Information Needs） / クエリ（Queries）**\n",
    "3. **関連性判断（Relevance Judgments）**\n",
    "\n",
    "- これらを組み合わせたセットを **テストコレクション（test collection）** と呼ぶ。\n",
    "- 各クエリと文書の組に対して「関連 or 非関連」を人手で判断。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔸 関連性の定義と注意点\n",
    "\n",
    "- 関連性は「クエリ」ではなく、背後の**情報ニーズ**に対して決定される。\n",
    "\n",
    "  例：  \n",
    "  - 情報ニーズ：赤ワインは白ワインより心臓病予防に有効か？  \n",
    "  - クエリ：`wine AND red AND white AND heart AND attack AND effective`\n",
    "\n",
    "- クエリ語を多く含む文書でも、ニーズに答えていなければ「非関連」。\n",
    "- 多くのクエリは情報ニーズを明示しないため、検索システムにとっては解釈が難しい。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔸 評価セットの規模と形式\n",
    "\n",
    "- 評価の信頼性を保つには、**少なくとも50個以上の情報ニーズ**が必要とされる。\n",
    "- 関連性は本来連続的な尺度（重要〜無関係）だが、実際には**バイナリ（二値）判断**がよく用いられる。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔸 評価とパラメータ調整（バイアス回避）\n",
    "\n",
    "- IRシステムは多くのパラメータ（例：重み付け）を持つ。\n",
    "- テストセットに合わせてパラメータを調整すると、**評価がバイアスされる（過学習）**。\n",
    "\n",
    "  → 解決策：  \n",
    "  - **開発用セット（development test collection）**でパラメータを調整  \n",
    "  - 本評価は **独立したテストセット** で行う\n",
    "\n",
    "---\n",
    "\n",
    "このセクションは、情報検索システムを **公正かつ再現性のある方法で評価**するための土台となる知識を提供しています。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ce2302",
   "metadata": {},
   "source": [
    "## 8.2 標準テストコレクション（Standard Test Collections）\n",
    "\n",
    "---\n",
    "\n",
    "### 🔸 標準テストコレクションの重要性\n",
    "\n",
    "- 情報検索システムの**客観的・再現可能な評価**を可能にする。\n",
    "- 研究者間での**公平な比較**や**ベンチマーク**に不可欠。\n",
    "- 自前の評価セットでは主観やバイアスが入りやすく、汎用性が低い。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Cranfield Collection（クランフィールド・コレクション）\n",
    "\n",
    "- 初の本格的IRテストコレクション（1950年代、英国）\n",
    "- 内容：\n",
    "  - 約1400件の工学系文書\n",
    "  - クエリとその関連文書は**人手で作成**\n",
    "- 成果：\n",
    "  - 様々な索引法や重み付け手法の評価に利用\n",
    "  - **逆文書頻度（IDF）の有効性**が確認された\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 TREC（Text REtrieval Conference）\n",
    "\n",
    "- 最も広く使われる大規模IR評価コンペ（1990年代〜、米NIST主催）\n",
    "- 特徴：\n",
    "  - 数十万〜数百万件の文書規模\n",
    "  - 数百のクエリ、数十の参加チーム\n",
    "- 意義：\n",
    "  - 実世界規模の文書（Web, News等）を対象とした評価が可能\n",
    "  - **新しい評価指標（MAP, NDCG など）**の開発にも寄与\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 その他の主要テストコレクション\n",
    "\n",
    "- **CLEF**：ヨーロッパ発の多言語IR評価（Cross-Language Evaluation Forum）\n",
    "- **NTCIR**：日本・アジア圏を対象としたIR評価（日本語あり）\n",
    "- **INEX**：XMLなど**構造化文書**の検索評価に特化\n",
    "- **OHSUMED**：医療・医学文書に特化（PubMedベース）\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 テストコレクションの利点と限界\n",
    "\n",
    "#### ✅ 利点\n",
    "\n",
    "- 評価の**客観性と再現性**が担保される\n",
    "- 複数システム間での**公平な比較**ができる\n",
    "- 評価指標の統一により**競争的な改良**が進む\n",
    "\n",
    "#### ⚠️ 限界\n",
    "\n",
    "- 関連性判断は主観的で完全ではない（判定者のばらつき）\n",
    "- 特定ジャンル・フォーマットに偏ると汎用性が落ちる\n",
    "- 実際のユーザの検索行動（インタラクション等）は考慮されていない\n",
    "\n",
    "---\n",
    "\n",
    "テストコレクションは、IRの研究・実験における**信頼できる評価基盤**であり、  \n",
    "システム性能の議論における**共通言語**としての役割を担っている。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8622ab",
   "metadata": {},
   "source": [
    "## 8.3 適合率と再現率（Precision and Recall）\n",
    "\n",
    "---\n",
    "\n",
    "### 🔸 情報検索における基本評価指標\n",
    "\n",
    "- 「良い検索結果」を数値で評価するために、**適合率（Precision）** と **再現率（Recall）** が用いられる。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 適合率（Precision）\n",
    "\n",
    "- **検索された文書のうち、実際に関連していた文書の割合**\n",
    "\n",
    "  ```text\n",
    "  Precision = Relevant documents retrieved / Total documents retrieved\n",
    "            = a / (a + c)\n",
    "  ```\n",
    "\n",
    "- 精度が高い → 検索結果にノイズ（無関係な文書）が少ない\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 再現率（Recall）\n",
    "\n",
    "- **関連する文書のうち、実際に検索された文書の割合**\n",
    "\n",
    "  ```text\n",
    "  Recall = Relevant documents retrieved / Total relevant documents in collection\n",
    "         = a / (a + b)\n",
    "  ```\n",
    "\n",
    "- 再現率が高い → 関係ある文書を取りこぼしていない\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 評価表（Confusion Matrix）\n",
    "\n",
    "|                             | 検索で取得 | 未取得 |\n",
    "|-----------------------------|------------|--------|\n",
    "| **関連あり（Relevant）**     | a          | b      |\n",
    "| **関連なし（Non-relevant）** | c          | d      |\n",
    "\n",
    "- a：正解（関連ありかつ取得）\n",
    "- b：見落とし（関連あるのに取得されなかった）\n",
    "- c：誤検出（関連ないのに取得された）\n",
    "- d：正しい無視（関連なしかつ取得されなかった）\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 トレードオフの関係\n",
    "\n",
    "- **Precision を上げると Recall が下がる**ことが多い（その逆も然り）。\n",
    "- 例：\n",
    "  - 絞り込みすぎ → Precision ↑ / Recall ↓\n",
    "  - 拡大しすぎ → Recall ↑ / Precision ↓\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 F値（F1スコア）\n",
    "\n",
    "- Precision と Recall の**バランスを取った調和平均**。\n",
    "- 定義：\n",
    "\n",
    "  ```text\n",
    "  F = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "  ```\n",
    "\n",
    "- 両者が高くないと F 値も上がらない → バランスの良いモデルを評価可能。\n",
    "\n",
    "---\n",
    "\n",
    "このセクションは、検索結果の良し悪しを定量的に測定するための**基本評価指標の理解**を提供しており、  \n",
    "IRシステムの評価における「第一歩」として必須の知識です。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc771ee",
   "metadata": {},
   "source": [
    "## 8.4 Precision at *k*（上位 *k* 件に対する適合率）\n",
    "\n",
    "---\n",
    "\n",
    "### 🔸 なぜ Precision@k を使うのか？\n",
    "\n",
    "- 実際のユーザーは検索結果の **最初の数件（例：1〜10件）しか見ない**。\n",
    "- よって、**全体の適合率よりも上位の結果の質が重要**。\n",
    "- それを測るのが Precision at *k*（略称：P@k）。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 定義：Precision at *k*\n",
    "\n",
    "- **上位 *k* 件の検索結果に含まれる関連文書の割合**\n",
    "\n",
    "  $text\n",
    "  P@k = （上位k件中の関連文書数） / k\n",
    "  $\n",
    "\n",
    "- 例：上位10件のうち7件が関連 → $P@10 = 0.7$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 特徴と利点\n",
    "\n",
    "- **シンプルで直感的**な評価指標。\n",
    "- 実際のユーザー行動（上位しか見ない）にマッチしている。\n",
    "- **ランキングモデルの性能**を簡易に評価できる。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 注意点・限界\n",
    "\n",
    "- *k* の値によって評価結果が変動する（例：P@1 と P@10 では意味が異なる）。\n",
    "- **関連文書の数が少ない場合**、P@kの上限が制限される。\n",
    "- 関連文書の位置や分布には敏感ではない。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 応用例と実務的な使われ方\n",
    "\n",
    "- **Web検索**：P@10 や P@5 がよく使われる。\n",
    "- **レコメンドシステム**：ユーザーが最初の数件しか確認しないため、Precision@k が重要。\n",
    "- **FAQ検索・チャットボット**：最初の候補が的確かどうかを評価。\n",
    "\n",
    "---\n",
    "\n",
    "Precision@k は、**現実的なユーザー体験を反映する実用的な評価指標**として広く用いられています。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958650c0",
   "metadata": {},
   "source": [
    "## 8.5 平均適合率（Average Precision, AP）\n",
    "\n",
    "---\n",
    "\n",
    "### 🔸 なぜ Average Precision（AP）を使うのか？\n",
    "\n",
    "- Precision@k は特定の位置（例：上位10件）のみを見る指標。\n",
    "- しかし、検索結果全体で「**関連文書が何位に現れるか**」も重要。\n",
    "- Average Precision は、**関連文書の出現位置すべてを考慮した評価指標**。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 定義：Average Precision (AP)\n",
    "\n",
    "- 関連文書が出現するたびにその時点の Precision を記録し、平均を取る：\n",
    "\n",
    "  $\n",
    "  AP = (1/R) * Σ P@k（関連文書が現れた順位 k における適合率）\n",
    "  $\n",
    "\n",
    "  - R：関連文書の総数\n",
    "  - P@k：k番目の文書が関連であったときの Precision\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 例\n",
    "\n",
    "検索結果：\n",
    "\n",
    "| 順位 | 文書 | 関連性 |\n",
    "|------|------|--------|\n",
    "| 1    | d1   | ✅     |\n",
    "| 2    | d2   | ❌     |\n",
    "| 3    | d3   | ✅     |\n",
    "| 4    | d4   | ❌     |\n",
    "| 5    | d5   | ✅     |\n",
    "\n",
    "- 関連文書数：3（d1, d3, d5）\n",
    "\n",
    "- Precision@1 = 1.0  \n",
    "  Precision@3 = 2/3  \n",
    "  Precision@5 = 3/5  \n",
    "\n",
    "- 計算：\n",
    "\n",
    "  $\n",
    "  AP = (1 + 2/3 + 3/5) / 3 ≈ 0.755\n",
    "  $\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 特徴と利点\n",
    "\n",
    "- **上位に関連文書が多く出るほど高評価**になる。\n",
    "- 単なる「数」ではなく、**順序（ランキング品質）**も評価可能。\n",
    "- 全体の精度とランキングのバランスを測るのに優れている。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 拡張と応用\n",
    "\n",
    "- 複数のクエリに対する AP の平均 → **MAP（Mean Average Precision）**\n",
    "- TREC や Web検索などでも標準的な評価指標。\n",
    "\n",
    "---\n",
    "\n",
    "Average Precision は、「どの位置に関連文書が現れたか」を重視する、  \n",
    "**ランキング評価のための基本的かつ強力な指標**です。\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
