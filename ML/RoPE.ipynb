{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "504918d4",
   "metadata": {},
   "source": [
    "# RoPE\n",
    "## 前提知識\n",
    "まずattention機構は次のように計算される。\n",
    "$$ \n",
    "\\bm{q}_m = f_q(\\bm{x}_m, m) \\\\\n",
    "\\bm{k}_n = f_k(\\bm{x}_n, n) \\\\\n",
    "\\bm{v}_n = f_v(\\bm{x}_n, n) \n",
    "$$\n",
    "入力ベクトルから上の値query, key, valueを定義。\n",
    "$$\n",
    "a_{m,n} = \\textrm{softmax}({\\frac{\\bm{q}_m^T \\bm{k}_n }{ \\sqrt{d}}}) \\\\\n",
    "\\bm{o}_m = \\sum^N_{n=1}a_{m,n}v_n\n",
    "$$\n",
    "RNNでは反復計算により入力ベクトルの順番から単語の位置の学習が行われたが、attentionを用いたアーキテクチャでは処理を並列で行うため単語の位置の学習が行われない。そのため入力ベクトルへの位置情報の付加(位置埋め込み:position embedding)が必要である。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199acf3a",
   "metadata": {},
   "source": [
    "## APE: Absolute position embedding（絶対位置埋め込み）\n",
    "各ベクトルにその語順の絶対的な位置を付加することをAbsolute Positional Embeddingという。\n",
    "### sinusoidal position embedding\n",
    "上の問題に対処するためにAttention is All You Needでは次のように正弦波によるpositional embeddingを行った。入力ベクトル$\\bm{x}_i$($i\\in\\{0,1,2,\\dots,(d/2)-1\\}$)に対し、\n",
    "$$\n",
    "f_{t:t\\in\\{ q,t,k\\}}(\\bm{x}_i, i) = W_{t:t\\in\\{ q,t,k\\}}(\\bm{x}_i+PE_i)\n",
    "$$\n",
    "ただし\n",
    "$$\n",
    "\\begin{align}\n",
    "PE_{i, 2t} = \\sin({i/10000^{2t/d}})\\\\ PE_{i, 2t+1} = \\cos({i/10000^{2t/d}})\n",
    "\\end{align}\n",
    "$$\n",
    "$PE_i$は入力ベクトルと同じ次元数であり、位置iに対して一意のベクトルである。これを実際に記述すると、\n",
    "$$ PE = [\\sin(i), \\cos(i), \\sin(\\frac{i}{10000^{2/d}}), \\cos(\\frac{i}{10000^{2/d}}), \\dots, \\sin(\\frac{i}{10000^{(d-2)/d}}), \\cos(\\frac{i}{10000^{(d-2)/d}})] $$\n",
    "![APE](img/APE.png)<br>\n",
    "三角関数を使う理由は収束と発散をせず、cosも用いる理由は違いを示すために周波数を小さくしたとき、sinの勾配が小さい時でもcosの勾配は大きくなるため。<br>\n",
    "attentionでは$\\bm{q}^T\\bm{k}$が単語間のベクトルの類似度を測る役割をしている（詳しくはattention matrix）。ここで行列計算として今回の計算をみると、\n",
    "$$\n",
    "QK^T = X \\times W_Q \\times W_K^T \\times X^T = X W X^T\n",
    "$$\n",
    "ここでWをひとまず考えずt番目の行とt'番目の列（二つのトークンの内積）を計算すると、\n",
    "$$\n",
    "X_tX_{t'}^T=(X_t+PE(t))(X_{t'}+PE(t'))^T=X_tX_{t'}^T+PE(t)X_{t'}+X_tPE(t')+PE(t)PE(t')\n",
    "$$\n",
    "ここで$X_tX_{t'}^T$は単純な単語の類似度を示しており、また、$PE(t)PE(t')$は\n",
    "$$\n",
    "PE(t)PE(t') = \\sin(t)\\sin(t')+\\cos(t)\\cos(t')+ \\dots + \\sin(\\frac{t}{10000^{(d-2)/d}})\\sin(\\frac{t'}{10000^{(d-2)/d}})+\\cos(\\frac{t}{10000^{(d-2)/d}})\\cos(\\frac{t'}{10000^{(d-2)/d}}) \\\\\n",
    "= \\sum\\cos(\\frac{1}{10000^{2i/d}}(t-t'))\n",
    "$$\n",
    "となりtとt'の位相の違いが表現できる。そのためsinusoidal position embeddingは2項、3項はわからないものの部分的に相対的な位置を表現できる可能性がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeec1b8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Relative position embedding: 相対位置埋め込み\n",
    "各ベクトルに相対的に考えられるように付加する方法が相対位置埋め込みである。<br>\n",
    "先ほどの式から2項と3項がないものを考えればより相対的な位置埋め込みが可能であると考えられる。つまり、$X$に先に可算をせず、$X_t X_{t'}$の積の後から位置情報を加算することで第2,3項を無くして考えられる（第4項の情報を形を変えて後から追加する）。\n",
    "$$\n",
    "QK^T = XW_QW_K^TX^T +b_{i,j}\n",
    "$$\n",
    "まだ詳しく調べてないので後ほど。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3076013",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## RoPE: Rotary position embedding（回転位置埋め込み）\n",
    "RoPEは相対位置埋め込みの一種だが、今までの延長線上のものではなく、queryとkeyに回転行列をかけて相対的な埋め込みをする。<br>\n",
    "まず、\n",
    "$$\n",
    "QK^T = XW_QR^{m-n}W_k^TX^T\n",
    "$$\n",
    "となるような時期が好ましい。そのため、次のような性質を満たす関数gとなるようなものを考えたい。\n",
    "$$\n",
    "\\langle f_q(\\bm{x}_m, m), f_k(\\bm{x}_n, n) \\rangle = g(\\bm{x}_m, \\bm{x}_n, m-n)\n",
    "$$\n",
    "\n",
    "### 二次の場合から考える\n",
    "次元数$d=2$、$\\thetaを0でない定数とする$\n",
    "$$\n",
    "f_q(\\bm{x}_m, m) = (\\bm{W}_q\\bm{x}_m)e^{im\\theta}\\\\\n",
    "f_k(\\bm{x}_n, n) = (\\bm{W}_k\\bm{x}_n)e^{in\\theta}\\\\\n",
    "g(\\bm{x}_m, \\bm{x}_n, m-n) = \\textrm{Re}[(\\bm{W}_q\\bm{x}_m)\\bm{W}_k\\bm{x}_ne^{i(m-n)\\theta}]\n",
    "$$\n",
    "そして$f_{\\{q,k\\}}$を二次の場合に考えると\n",
    "$$\n",
    "f_{\\{q,k\\}}(x_m, m) =\n",
    "\\begin{pmatrix}\n",
    "\\cos(m\\theta) & -\\sin(m\\theta) \\\\\n",
    "\\sin(m\\theta) & \\cos(m\\theta)\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "W_{\\{q,k\\}}^{(11)} & W_{\\{q,k\\}}^{(12)} \\\\\n",
    "W_{\\{q,k\\}}^{(21)} & W_{\\{q,k\\}}^{(22)}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x_m^{(1)} \\\\\n",
    "x_m^{(2)}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "とする。\n",
    "\n",
    "### 二次から一般形への拡張"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15222ba4",
   "metadata": {},
   "source": [
    "## 参考url\n",
    "- https://arxiv.org/abs/2104.09864\n",
    "- https://www.youtube.com/watch?v=SMBkImDWOyQ (概要、おすすめ)\n",
    "- https://www.nomuyu.com/positional-encoding/　(APE、おすすめ)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
