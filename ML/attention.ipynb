{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3c5477",
   "metadata": {},
   "source": [
    "# Attention\n",
    "attentionとは様々な場所で用いられている。例を挙げるとするならば\n",
    "- CNNの画像認識\n",
    "- seq2seqの機械翻訳\n",
    "- Transformerの機械翻訳\n",
    "- Vision Transformerの画像認識\n",
    "  \n",
    "# Attentionの数式\n",
    "$$ Attention(Q, K, V) = softmax(\\frac{QK^t}{\\sqrt{d_k}})V $$ \n",
    "入力ベクトル$X=[x_1, x_2, \\dots, x_L]$<br>\n",
    "（例えば\"I have a pen\"ならば$x_1$=\"I\"のベクトル, $x_2$=\"have\"のベクトル）<br>\n",
    "$$  Q = XW^Q \\\\\n",
    "    K = XW^K \\\\\n",
    "    V = XW^V $$\n",
    "（$W^Q,W^K,W^V$は学習される重み行列）<br>\n",
    "$Q$:n個のクエリ、$(V,K)$:keyとvalueでm個のペア\n",
    "$$ Q = \\begin{pmatrix}\n",
    " q_1 \\\\ \\vdots \\\\ q_n  \n",
    "\\end{pmatrix} $$\n",
    "$$ K=\\begin{pmatrix}\n",
    "k_1 \\\\ \\vdots \\\\ k_m\n",
    "\\end{pmatrix} $$\n",
    "$$ V = \\begin{pmatrix}\n",
    "v_1 \\\\ \\vdots \\\\ v_m\n",
    "\\end{pmatrix} $$\n",
    "図で表すと<br>\n",
    "![attention](img/attention.avif)\n",
    "\n",
    "つまり、\n",
    "\n",
    "\n",
    "## Attentionのメリット\n",
    "RNNにくらべ、\n",
    "- 性能がいい\n",
    "- GPUをフルに使える<br>\n",
    "RNNは時刻tの計算が終わらないと時刻t+1の計算が行えないが、attentionは全ての計算を行える。\n",
    "- 構造がシンプル<br>\n",
    "全結合と行列積で理解可能\n",
    "\n",
    "## 参考文献\n",
    "- https://www.youtube.com/watch?v=bPdyuIebXWM\n",
    "- https://www.youtube.com/watch?v=50XvMaWhiTY\n",
    "- https://qiita.com/ps010/items/0bb2931b666fa602d0fc\n",
    "- https://qiita.com/halhorn/items/c91497522be27bde17ce\n",
    "- https://arxiv.org/abs/1409.0473\n",
    "- https://deeplearning.hatenablog.com/entry/transformer (おすすめ)\n",
    "- https://zenn.dev/bilzard/articles/49d453c7809be3\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
